# Production RAG Optimization

## Overview

Creating RAG systems is straightforward initially, but scaling them to handle complex knowledge bases while maintaining performance and accuracy requires sophisticated techniques. This guide outlines production-grade optimization strategies for real-world deployments.

## Core Optimization Principles

### 1. Decouple Retrieval and Synthesis Chunks

**Problem**: "The optimal chunk representation for retrieval might be different than the optimal context used for synthesis."

**Issues**:
- Raw chunks may contain noise affecting embeddings
- Raw chunks may lack sufficient context for effective retrieval
- Synthesis needs detailed information, retrieval needs precision

**Solutions**:

**Document Summary Linking**:
- Embed high-level document summaries that reference associated chunks
- Two-stage retrieval: document-level first, then chunk-level
- Reduces irrelevant chunk retrieval from off-topic documents

**Sentence-Window Technique**:
- Embed individual sentences for fine-grained retrieval
- Link to surrounding context windows for synthesis
- Addresses "lost in the middle" problem with large chunks

### 2. Structured Retrieval for Scale

**Challenge**: Standard RAG (top-k similarity + text splitting) degrades with scale, particularly across 100+ documents. Semantic similarity alone cannot discriminate when multiple documents are superficially relevant.

**Implementation Approaches**:

**Metadata Filtering with Auto-Retrieval**:

```python
from llama_index.core.vector_stores import MetadataFilters, ExactMatchFilter

filters = MetadataFilters(
    filters=[
        ExactMatchFilter(key="category", value="technical"),
        ExactMatchFilter(key="year", value="2024")
    ]
)

query_engine = index.as_query_engine(filters=filters)
```

**Advantages**:
- Vector database support across major platforms
- Multi-dimensional document filtering
- Keyword-level precision at document level

**Limitations**:
- Tag definition complexity
- Limited semantic expressiveness
- Metadata may be insufficient for nuanced retrieval

**Document Hierarchy with Recursive Retrieval**:

```python
from llama_index.core import DocumentSummaryIndex

summary_index = DocumentSummaryIndex.from_documents(documents)
retriever = summary_index.as_retriever()
```

**Advantages**:
- Semantic document-level lookups
- Natural hierarchical structure preservation

**Limitations**:
- No keyword filtering capability
- Summary generation computational cost

### 3. Task-Based Dynamic Retrieval

**Use Case Diversity**: RAG systems must handle varied query types:

- **Factual queries**: "What were the Q3 financial metrics?"
- **Summarization**: "Provide a high-level overview"
- **Comparison**: "Contrast approach X with approach Y"
- **Multi-source synthesis**: Combining structured and unstructured data

**Core Abstractions**:

**Router Module**: Directs queries to specialized handlers

```python
from llama_index.core.query_engine import RouterQueryEngine
from llama_index.core.selectors import LLMSingleSelector

query_engine = RouterQueryEngine(
    selector=LLMSingleSelector.from_defaults(),
    query_engine_tools=[summary_tool, vector_tool, sql_tool]
)
```

**Sub-Question Engine**: Decompose complex queries

```python
from llama_index.core.query_engine import SubQuestionQueryEngine

query_engine = SubQuestionQueryEngine.from_defaults(
    query_engine_tools=query_engine_tools,
    use_async=True
)
```

**Agents**: Autonomous query routing and tool selection

```python
from llama_index.core.agent import ReActAgent

agent = ReActAgent.from_tools(tools, llm=llm)
```

### 4. Optimize Context Embeddings

**Rationale**: Pre-trained embedding models may not capture domain-specific relevance. Custom optimization improves retrieval quality over specialized corpora.

**Approach**:
- Label-free fine-tuning over unstructured text
- Domain adaptation for specific data distributions

## Ingestion Pipeline Optimization

### Architecture & Caching

```python
from llama_index.core.ingestion import IngestionPipeline

pipeline = IngestionPipeline(
    transformations=[
        SentenceSplitter(chunk_size=512),
        TitleExtractor(),
        OpenAIEmbedding(),
    ]
)
```

**Key Feature**: "Each node+transformation pair is hashed and cached, so subsequent runs...with the same node+transformation combination can use the cached result."

### Local Cache Management

```python
# Persist cache to disk
pipeline.persist("./pipeline_storage")

# Load and restore
new_pipeline = IngestionPipeline(transformations=[...])
new_pipeline.load("./pipeline_storage")

# Subsequent runs leverage cached results
nodes = pipeline.run(documents=documents)
```

**When to clear cache**:
```python
cache.clear()  # When storage becomes excessive
```

### Remote Cache (Production)

**Redis Cache**:
```python
from llama_index.ingestion import IngestionCache
from llama_index.storage.kvstore.redis import RedisKVStore

ingest_cache = IngestionCache(
    cache=RedisKVStore.from_host_and_port(host="127.0.0.1", port=6379),
    collection="my_cache"
)

pipeline = IngestionPipeline(
    transformations=[...],
    cache=ingest_cache
)
```

**Supported Backends**:
- RedisCache (distributed)
- MongoDBCache (document-based)
- FirestoreCache (cloud-native)

### Parallel Processing

```python
# Distribute across CPU cores
pipeline.run(documents=documents, num_workers=4)
```

Uses `multiprocessing.Pool` for parallelization.

### Async Support

```python
nodes = await pipeline.arun(documents=documents)
```

## Data Loading Optimization

### Parallel File Loading

**Sequential vs. Parallel Performance** (32 PDF files, 4,306 documents):

| Method | Time | Speedup |
|--------|------|---------|
| Sequential | 391s | 1x |
| Parallel (10 workers) | 31s | 13x |

```python
from llama_index.core import SimpleDirectoryReader

reader = SimpleDirectoryReader(input_dir="./data/source_files")
documents = reader.load_data(num_workers=10)
```

**Platform Note**: Windows users may see diminished performance gains vs. Linux/macOS.

### Document Management

Attach document store to detect duplicates:

```python
from llama_index.core.storage.docstore import SimpleDocumentStore

pipeline = IngestionPipeline(
    transformations=[...],
    docstore=SimpleDocumentStore()
)
```

**Behavior**:
- Maps document IDs to content hashes
- With vector store: Reupserts changed documents, skips unchanged
- Without vector store: Detects duplicates across all content

## Vector Store Integration

### Direct Pipeline Integration

```python
from llama_index.vector_stores.qdrant import QdrantVectorStore
import qdrant_client

client = qdrant_client.QdrantClient(location=":memory:")
vector_store = QdrantVectorStore(client=client, collection_name="test_store")

pipeline = IngestionPipeline(
    transformations=[SentenceSplitter(), OpenAIEmbedding()],
    vector_store=vector_store
)

pipeline.run(documents=documents)
index = VectorStoreIndex.from_vector_store(vector_store)
```

**Note**: Embeddings must be included in pipeline when connecting to vector store.

### Vector Store Best Practices

1. **Batch Operations**: Use bulk upsert for large datasets
2. **Index Configuration**: Configure appropriate index types (HNSW, IVF)
3. **Metadata Schema**: Design metadata fields upfront
4. **Partitioning**: Use collections/namespaces for multi-tenancy
5. **Backup Strategy**: Regular snapshots of vector data

## Performance Scaling

### Memory Optimization

1. **Batch Size Control**:
   ```python
   embed_model = OpenAIEmbedding(embed_batch_size=50)
   ```

2. **Streaming Processing**:
   ```python
   for batch in document_batches:
       nodes = pipeline.run(documents=batch)
       # Process and clear
   ```

3. **Generator Pattern**:
   ```python
   def document_generator():
       for doc in large_corpus:
           yield doc
   ```

### Latency Optimization

1. **Fast Metadata Pre-filtering**:
   - Build inverted indices for categorical fields
   - Use B-tree indices for numeric fields
   - Filter before vector search (90% reduction possible)

2. **Caching Strategies**:
   - Cache frequent queries
   - Cache document summaries
   - Cache embeddings with TTL

3. **Async Operations**:
   ```python
   # Parallel retrieval
   retriever = index.as_retriever(use_async=True)
   ```

### Cost Optimization

1. **Local Embeddings**:
   ```python
   # Replace API with local model
   Settings.embed_model = HuggingFaceEmbedding(
       model_name="BAAI/bge-base-en-v1.5",
       backend="onnx"  # 3-7x faster
   )
   ```

2. **Batch Processing**:
   - Reduce per-request overhead
   - Maximize throughput

3. **Smart Caching**:
   - Reuse embeddings across runs
   - Cache LLM responses for common queries

4. **Selective Reindexing**:
   ```python
   # Only process changed documents
   pipeline = IngestionPipeline(
       transformations=[...],
       docstore=docstore  # Tracks changes
   )
   ```

## Monitoring & Observability

### Key Metrics

**Performance Metrics**:
- Retrieval latency (p50, p95, p99)
- Embedding generation time
- Query processing time
- Cache hit rate

**Quality Metrics**:
- Hit rate (retrieval accuracy)
- MRR (ranking quality)
- Response faithfulness
- Context relevancy

**Cost Metrics**:
- Embedding API calls
- LLM API calls
- Token usage
- Compute costs

### Callback Integration

```python
from llama_index.core.callbacks import CallbackManager, LlamaDebugHandler

llama_debug = LlamaDebugHandler(print_trace_on_end=True)
callback_manager = CallbackManager([llama_debug])
Settings.callback_manager = callback_manager

# Detailed execution traces
response = query_engine.query(query_str)
```

### Production Monitoring

```python
import time

class MetricsCallback:
    def __init__(self):
        self.metrics = {
            "retrieval_latency": [],
            "llm_calls": 0,
            "token_usage": 0
        }

    def on_event_start(self, event_type, payload):
        payload["start_time"] = time.time()

    def on_event_end(self, event_type, payload):
        latency = time.time() - payload["start_time"]
        self.metrics["retrieval_latency"].append(latency)
```

## Multi-Tenancy RAG

**Purpose**: Ensure data security by restricting document access to authorized users only.

**Key Benefit**: "Search operations are confined to the user's own data, protecting sensitive information."

**Implementation**:

```python
from llama_index.core.vector_stores import MetadataFilters, ExactMatchFilter

# User-specific filtering
user_filters = MetadataFilters(
    filters=[ExactMatchFilter(key="user_id", value=current_user_id)]
)

query_engine = index.as_query_engine(filters=user_filters)
```

**Best Practices**:
1. **Metadata Design**: Include user_id, org_id in all documents
2. **Filter Enforcement**: Apply filters at query time
3. **Collection Isolation**: Use separate collections per tenant for large datasets
4. **Access Control**: Validate user permissions before queries

## Deployment Patterns

### Serverless Deployment

**Advantages**:
- Auto-scaling
- Pay-per-use
- Zero infrastructure management

**Considerations**:
- Cold start latency
- Embedding model size limits
- Stateless design required

### Container-Based Deployment

```dockerfile
FROM python:3.10-slim

# Install dependencies
COPY requirements.txt .
RUN pip install -r requirements.txt

# Copy application
COPY . /app
WORKDIR /app

# Run application
CMD ["python", "main.py"]
```

**Advantages**:
- Consistent environments
- Easy scaling
- Local embedding models

### Microservices Architecture

```
┌─────────────┐     ┌──────────────┐     ┌─────────────┐
│  Ingestion  │────>│ Vector Store │<────│  Retrieval  │
│   Service   │     │   Service    │     │   Service   │
└─────────────┘     └──────────────┘     └─────────────┘
       │                                         │
       v                                         v
┌─────────────┐                         ┌─────────────┐
│   Cache     │                         │ Query Engine│
│   Service   │                         │   Service   │
└─────────────┘                         └─────────────┘
```

**Benefits**:
- Independent scaling
- Service isolation
- Technology flexibility

## Error Handling & Reliability

### Retry Logic

```python
import tenacity

@tenacity.retry(
    wait=tenacity.wait_exponential(multiplier=1, min=2, max=10),
    stop=tenacity.stop_after_attempt(3)
)
def embed_with_retry(text):
    return embed_model.get_text_embedding(text)
```

### Fallback Strategies

```python
def retrieve_with_fallback(query):
    try:
        # Primary: Hybrid search
        return hybrid_retriever.retrieve(query)
    except Exception:
        try:
            # Fallback 1: Vector only
            return vector_retriever.retrieve(query)
        except Exception:
            # Fallback 2: BM25
            return bm25_retriever.retrieve(query)
```

### Circuit Breaker Pattern

```python
from pybreaker import CircuitBreaker

breaker = CircuitBreaker(fail_max=5, timeout_duration=60)

@breaker
def call_embedding_api(text):
    return openai_embed(text)
```

## Testing Strategy

### Unit Tests

```python
def test_retrieval_accuracy():
    retriever = index.as_retriever(similarity_top_k=5)
    results = retriever.retrieve("test query")
    assert len(results) == 5
    assert all(r.score > 0 for r in results)
```

### Integration Tests

```python
async def test_full_pipeline():
    # Load documents
    documents = SimpleDirectoryReader("./test_data").load_data()

    # Run pipeline
    pipeline = IngestionPipeline(transformations=[...])
    nodes = await pipeline.arun(documents=documents)

    # Verify results
    assert len(nodes) > 0
    assert all(hasattr(n, "embedding") for n in nodes)
```

### Load Tests

```python
import asyncio
from locust import HttpUser, task, between

class RAGLoadTest(HttpUser):
    wait_time = between(1, 3)

    @task
    def query_endpoint(self):
        self.client.post("/query", json={"query": "test"})
```

## Relevance to Your Pipelines

### Current State Analysis

**src-iLand/**:
- ✅ Modular pipeline architecture
- ✅ Batch embedding with caching
- ✅ Fast metadata filtering (<50ms)
- ⚠️ No parallel loading
- ⚠️ No remote cache (Redis)
- ⚠️ Limited monitoring

**src/**:
- ✅ Multiple retrieval strategies
- ⚠️ No unified pipeline
- ⚠️ No caching layer
- ⚠️ Sequential processing

### Optimization Opportunities

1. **Enable Parallel Loading**:
   ```python
   # In data_processing
   documents = SimpleDirectoryReader(input_dir).load_data(num_workers=4)
   ```

2. **Add Redis Caching**:
   ```python
   # For production deployments
   ingest_cache = IngestionCache(
       cache=RedisKVStore.from_host_and_port(host="redis", port=6379)
   )
   ```

3. **Implement Monitoring**:
   ```python
   # Track metrics across all retrievers
   callback_manager = CallbackManager([MetricsCallback()])
   ```

4. **Add Retry Logic**:
   ```python
   # Wrap embedding calls
   @tenacity.retry(stop=tenacity.stop_after_attempt(3))
   def generate_embeddings(texts):
       return embed_model.get_text_embeddings(texts)
   ```

## References

- [Production RAG Guide](https://developers.llamaindex.ai/python/framework/optimizing/production_rag/)
- [Ingestion Pipeline Documentation](https://developers.llamaindex.ai/python/framework/module_guides/loading/ingestion_pipeline)
- [SimpleDirectoryReader Parallel](https://developers.llamaindex.ai/python/examples/data_connectors/simple_directory_reader_parallel/)
- [Building RAG from Scratch](https://developers.llamaindex.ai/python/framework/optimizing/building_rag_from_scratch/)
