#!/usr/bin/env python3
"""
Validate RAG configuration before deployment.

Checks:
- Chunk size appropriate for domain
- Embedding model consistency
- Top-k values reasonable
- Reranker configuration

Usage:
    python validate_config.py --chunk-size 1024 --top-k 5 --embedding-model text-embedding-3-small
"""

import argparse
import sys


def validate_chunk_size(chunk_size: int, domain: str = "general") -> tuple[bool, str]:
    """Validate chunk size is appropriate for domain."""
    recommendations = {
        "legal": (512, 1024),
        "technical": (512, 1024),
        "narrative": (1024, 2048),
        "qa": (256, 512),
        "general": (512, 2048)
    }

    min_size, max_size = recommendations.get(domain, (512, 2048))

    if chunk_size < 128:
        return False, f"Chunk size {chunk_size} is too small. Minimum recommended: {min_size}"
    elif chunk_size > 4096:
        return False, f"Chunk size {chunk_size} exceeds most embedding model limits (4096 tokens)"
    elif chunk_size < min_size or chunk_size > max_size:
        return True, f"Warning: Chunk size {chunk_size} outside recommended range [{min_size}, {max_size}] for {domain} domain"
    else:
        return True, f"✓ Chunk size {chunk_size} appropriate for {domain} domain"


def validate_top_k(top_k: int, chunk_size: int) -> tuple[bool, str]:
    """Validate top-k value is reasonable."""
    if top_k < 1:
        return False, "top_k must be at least 1"
    elif top_k > 50:
        return False, f"top_k={top_k} is very high, may include irrelevant results"
    elif top_k < 3:
        return True, f"Warning: top_k={top_k} is low, may miss relevant results. Consider 5-10"
    else:
        # Check rule of thumb: when halving chunk size, double top_k
        # Base assumption: 1024 chunk → 5 top_k
        expected_top_k = max(3, int(5 * (1024 / chunk_size)))
        if abs(top_k - expected_top_k) > 3:
            return True, f"Consider top_k={expected_top_k} for chunk_size={chunk_size} (current: {top_k})"
        return True, f"✓ top_k={top_k} appropriate for chunk_size={chunk_size}"


def validate_embedding_model(model_name: str) -> tuple[bool, str]:
    """Validate embedding model is recognized."""
    known_models = {
        # OpenAI
        "text-embedding-3-small": "OpenAI (good quality, cost-effective)",
        "text-embedding-3-large": "OpenAI (highest quality)",
        "text-embedding-ada-002": "OpenAI (older, still good)",

        # HuggingFace
        "BAAI/bge-small-en-v1.5": "HuggingFace (fast, local)",
        "BAAI/bge-base-en-v1.5": "HuggingFace (balanced, local)",
        "BAAI/bge-large-en-v1.5": "HuggingFace (best quality, local)",

        # Cohere
        "embed-english-v3.0": "Cohere (good quality)",
        "embed-multilingual-v3.0": "Cohere (multi-language support)",

        # JinaAI
        "jina-embeddings-v2-base-en": "JinaAI (balanced)",
        "jina-embeddings-v2-small-en": "JinaAI (fast)",
    }

    if model_name in known_models:
        return True, f"✓ {model_name}: {known_models[model_name]}"
    else:
        return True, f"Warning: Unrecognized model '{model_name}'. Ensure it's valid for your embedding provider"


def validate_reranker(reranker_model: str = None, retrieve_top_k: int = None, rerank_top_n: int = None) -> tuple[bool, str]:
    """Validate reranker configuration."""
    if not reranker_model:
        return True, "No reranker configured (optional but recommended for production)"

    known_rerankers = {
        "rerank-english-v3.0": "Cohere (best quality, API)",
        "rerank-multilingual-v3.0": "Cohere (multi-language, API)",
        "BAAI/bge-reranker-large": "HuggingFace (best open-source, local)",
        "BAAI/bge-reranker-base": "HuggingFace (balanced, local)",
        "cross-encoder/ms-marco-MiniLM-L-6-v2": "SentenceTransformer (fast, local)",
    }

    messages = []

    # Check model
    if reranker_model in known_rerankers:
        messages.append(f"✓ Reranker: {reranker_model} ({known_rerankers[reranker_model]})")
    else:
        messages.append(f"Warning: Unrecognized reranker '{reranker_model}'")

    # Check retrieve vs rerank ratio
    if retrieve_top_k and rerank_top_n:
        if retrieve_top_k < rerank_top_n:
            messages.append(f"Error: retrieve_top_k ({retrieve_top_k}) must be >= rerank_top_n ({rerank_top_n})")
            return False, "\n".join(messages)
        elif retrieve_top_k == rerank_top_n:
            messages.append(f"Warning: retrieve_top_k ({retrieve_top_k}) equals rerank_top_n ({rerank_top_n}). Reranking has no effect. Recommend retrieve_top_k = 2-3x rerank_top_n")
        elif retrieve_top_k < rerank_top_n * 1.5:
            messages.append(f"Warning: retrieve_top_k ({retrieve_top_k}) only {retrieve_top_k/rerank_top_n:.1f}x rerank_top_n ({rerank_top_n}). Recommend 2-3x for better reranking")
        else:
            messages.append(f"✓ retrieve_top_k ({retrieve_top_k}) is {retrieve_top_k/rerank_top_n:.1f}x rerank_top_n ({rerank_top_n})")

    return True, "\n".join(messages)


def main():
    parser = argparse.ArgumentParser(description="Validate RAG configuration")
    parser.add_argument("--chunk-size", type=int, default=1024, help="Document chunk size in tokens")
    parser.add_argument("--chunk-overlap", type=int, default=20, help="Chunk overlap in tokens")
    parser.add_argument("--top-k", type=int, default=5, help="Number of results to retrieve")
    parser.add_argument("--embedding-model", type=str, default="text-embedding-3-small", help="Embedding model name")
    parser.add_argument("--domain", type=str, default="general", choices=["legal", "technical", "narrative", "qa", "general"], help="Document domain")
    parser.add_argument("--reranker-model", type=str, help="Reranker model name (optional)")
    parser.add_argument("--retrieve-top-k", type=int, help="Top-k for initial retrieval before reranking")
    parser.add_argument("--rerank-top-n", type=int, help="Final top-n after reranking")

    args = parser.parse_args()

    print("=" * 60)
    print("RAG Configuration Validation")
    print("=" * 60)

    all_valid = True

    # Validate chunk size
    valid, message = validate_chunk_size(args.chunk_size, args.domain)
    print(f"\n[Chunk Size]\n{message}")
    if not valid:
        all_valid = False

    # Validate chunk overlap
    if args.chunk_overlap < 0:
        print(f"\n[Chunk Overlap]\nError: chunk_overlap must be >= 0")
        all_valid = False
    elif args.chunk_overlap >= args.chunk_size:
        print(f"\n[Chunk Overlap]\nError: chunk_overlap ({args.chunk_overlap}) must be < chunk_size ({args.chunk_size})")
        all_valid = False
    elif args.chunk_overlap < args.chunk_size * 0.05:
        print(f"\n[Chunk Overlap]\nWarning: overlap ({args.chunk_overlap}) is < 5% of chunk_size. Consider 10-20% for better context")
    else:
        overlap_pct = (args.chunk_overlap / args.chunk_size) * 100
        print(f"\n[Chunk Overlap]\n✓ Overlap {args.chunk_overlap} ({overlap_pct:.1f}% of chunk size)")

    # Validate top-k
    valid, message = validate_top_k(args.top_k, args.chunk_size)
    print(f"\n[Top-K]\n{message}")
    if not valid:
        all_valid = False

    # Validate embedding model
    valid, message = validate_embedding_model(args.embedding_model)
    print(f"\n[Embedding Model]\n{message}")
    if not valid:
        all_valid = False

    # Validate reranker
    valid, message = validate_reranker(args.reranker_model, args.retrieve_top_k, args.rerank_top_n)
    print(f"\n[Reranker]\n{message}")
    if not valid:
        all_valid = False

    # Final verdict
    print("\n" + "=" * 60)
    if all_valid:
        print("✓ Configuration validation passed")
        print("=" * 60)
        return 0
    else:
        print("✗ Configuration validation failed - please review errors above")
        print("=" * 60)
        return 1


if __name__ == "__main__":
    sys.exit(main())
