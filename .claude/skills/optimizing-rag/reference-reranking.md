# Reranking and Node Postprocessors

## Overview

Node postprocessors refine retrieved results before passing them to response synthesizers. They filter, rerank, or modify nodes to improve answer quality. Reranking is particularly powerful—nearly all embeddings benefit from reranking, showing improved hit rates and MRRs.

## Why Reranking Matters

**Key Insight**: "Nearly all embeddings benefit from reranking, showing improved hit rates and MRRs"—rerankers transform any embedding into a competitive option.

**Performance Impact**: Reranking can improve hit rates by 5-15% and MRR by 3-10% across different embedding models.

## Reranking Models

### 1. CohereRerank (Recommended)

Commercial API-based reranking with state-of-the-art performance.

```python
from llama_index.postprocessor.cohere_rerank import CohereRerank

reranker = CohereRerank(
    top_n=5,
    model="rerank-english-v3.0",  # or rerank-multilingual-v3.0
    api_key="YOUR_COHERE_API_KEY"
)

query_engine = index.as_query_engine(
    similarity_top_k=10,
    node_postprocessors=[reranker]
)
```

**Performance with Different Embeddings**:
- OpenAI + CohereRerank: Hit Rate 0.927, MRR 0.866
- JinaAI Base + CohereRerank: Hit Rate 0.933, MRR 0.874
- bge-large + CohereRerank: Hit Rate 0.876, MRR 0.823

**Strengths**:
- Best overall performance
- Multi-language support (v3.0 multilingual)
- Easy integration

**Limitations**:
- API costs
- Latency overhead
- External dependency

### 2. SentenceTransformerRerank (Local)

Cross-encoder models for local reranking with no API calls.

```python
from llama_index.postprocessor.sentence_transformer_rerank import SentenceTransformerRerank

reranker = SentenceTransformerRerank(
    model="cross-encoder/ms-marco-MiniLM-L-2-v2",
    top_n=5
)
```

**Popular Models**:
- `cross-encoder/ms-marco-MiniLM-L-2-v2` - Fast, lightweight
- `cross-encoder/ms-marco-MiniLM-L-6-v2` - Balanced
- `cross-encoder/ms-marco-MiniLM-L-12-v2` - Higher quality

**Strengths**:
- No API costs
- Local inference
- Fast for small result sets

**Limitations**:
- Lower accuracy than CohereRerank
- Computational overhead for large result sets

### 3. bge-reranker (Open Source)

BAAI's open-source reranker models.

```python
from llama_index.postprocessor.flag_embedding_reranker import FlagEmbeddingReranker

reranker = FlagEmbeddingReranker(
    model="BAAI/bge-reranker-large",
    top_n=5
)
```

**Available Models**:
- `BAAI/bge-reranker-base` - Fast, good quality
- `BAAI/bge-reranker-large` - Best open-source quality

**Performance**:
- OpenAI + bge-reranker-large: Hit Rate 0.910, MRR 0.856
- JinaAI Base + bge-reranker-large: Hit Rate 0.938, MRR 0.869

**Strengths**:
- Excellent open-source performance
- No API costs
- Good alternative to CohereRerank

### 4. JinaRerank

Jina AI's reranking service.

```python
from llama_index.postprocessor.jinaai_rerank import JinaRerank

reranker = JinaRerank(
    top_n=5,
    model="jina-reranker-v1-base-en",
    api_key="YOUR_JINA_API_KEY"
)
```

**Strengths**:
- Good performance
- API-based convenience

**Limitations**:
- API costs
- Less widely adopted than Cohere

### 5. ColbertRerank

Fine-grained token-level similarity matching using ColBERT v2.

```python
from llama_index.postprocessor.colbert_rerank import ColbertRerank

colbert_reranker = ColbertRerank(
    top_n=5,
    model="colbert-ir/colbertv2.0",
    keep_retrieval_score=True
)
```

**Strengths**:
- Token-level matching precision
- Better for technical/scientific content
- Local inference

**Limitations**:
- Higher computational cost
- Requires more memory

### 6. LLM-Based Reranking

Use LLMs to score and order nodes by relevance.

```python
from llama_index.core.postprocessor import LLMRerank

reranker = LLMRerank(
    top_n=5,
    service_context=service_context
)
```

**RankGPT** (Specialized):
```python
from llama_index.postprocessor.rankgpt_rerank import RankGPTRerank

reranker = RankGPTRerank(
    top_n=5,
    llm=OpenAI(model="gpt-3.5-turbo-16k")
)
```

**Strengths**:
- Leverages LLM reasoning
- Can handle complex relevance criteria

**Limitations**:
- High latency
- Expensive (LLM API calls)
- Variable quality

## Non-Reranking Postprocessors

### Similarity Filtering

Remove nodes below similarity threshold:

```python
from llama_index.core.postprocessor import SimilarityPostprocessor

postprocessor = SimilarityPostprocessor(similarity_cutoff=0.7)
```

**Use case**: Filter out marginally relevant results.

### Keyword Filtering

Enforce term inclusion/exclusion:

```python
from llama_index.core.postprocessor import KeywordNodePostprocessor

postprocessor = KeywordNodePostprocessor(
    required_keywords=["machine learning", "neural networks"],
    exclude_keywords=["deprecated", "legacy"]
)
```

**Use case**: Ensure results contain specific terms.

### Metadata Replacement

Substitute node content with metadata fields (chunk decoupling):

```python
from llama_index.core.postprocessor import MetadataReplacementPostProcessor

postprocessor = MetadataReplacementPostProcessor(
    target_metadata_key="window"
)
```

**Use case**: Sentence-window retrieval pattern.

### Sentence Embedding Optimizer

Reduce token usage by removing query-irrelevant sentences:

```python
from llama_index.core.postprocessor import SentenceEmbeddingOptimizer

postprocessor = SentenceEmbeddingOptimizer(
    embed_model=embed_model,
    percentile_cutoff=0.5  # or threshold_cutoff=0.7
)
```

**Use case**: Optimize context length for LLM.

### Long Context Reorder

Address "lost in the middle" phenomenon by repositioning crucial nodes:

```python
from llama_index.core.postprocessor import LongContextReorder

postprocessor = LongContextReorder()
```

**Why it matters**: "Models struggle to access significant details found in the center of extended contexts."

**Use case**: Long context windows with many retrieved nodes.

### PII Removal

Mask personally identifiable information:

```python
# LLM-based PII masking
from llama_index.postprocessor.pii import PIINodePostprocessor

postprocessor = PIINodePostprocessor()
```

```python
# NER-based PII masking (local)
from llama_index.postprocessor.pii import NERPIINodePostprocessor

postprocessor = NERPIINodePostprocessor()
```

**Use case**: Privacy compliance, data anonymization.

### Temporal Postprocessors

**FixedRecencyPostprocessor**: Prioritize by date metadata.

**EmbeddingRecencyPostprocessor**: Combine date sorting with similarity filtering.

**TimeWeightedPostprocessor**: Apply time decay to favor recent retrievals.

**Use case**: Time-sensitive information, news, evolving knowledge bases.

## Integration Patterns

### Single Postprocessor

```python
query_engine = index.as_query_engine(
    similarity_top_k=10,
    node_postprocessors=[reranker]
)
```

### Multiple Postprocessors (Chain)

Progressive refinement—filtering, reranking, then metadata replacement:

```python
query_engine = index.as_query_engine(
    similarity_top_k=10,
    node_postprocessors=[
        SimilarityPostprocessor(similarity_cutoff=0.7),
        CohereRerank(top_n=5),
        MetadataReplacementPostProcessor(target_metadata_key="window")
    ]
)
```

### Custom Retriever Integration

```python
from llama_index.core.query_engine import RetrieverQueryEngine

custom_retriever = CustomRetriever(vector_retriever)
query_engine = RetrieverQueryEngine.from_args(
    retriever=custom_retriever,
    node_postprocessors=[reranker]
)
```

## Reranking Best Practices

### 1. Always Retrieve More Than You Need

```python
# Retrieve 10x, rerank to top 5
query_engine = index.as_query_engine(
    similarity_top_k=10,  # Initial retrieval
    node_postprocessors=[
        CohereRerank(top_n=5)  # Final results
    ]
)
```

**Rationale**: Gives reranker diverse candidates to choose from.

### 2. Choose Reranker Based on Use Case

**Production quality**: CohereRerank or bge-reranker-large
**Cost-sensitive**: SentenceTransformerRerank or bge-reranker-base
**Multi-language**: CohereRerank multilingual
**Technical content**: ColbertRerank

### 3. Evaluate on Your Data

Different rerankers excel with different embeddings and domains:

```python
rerankers = [
    CohereRerank(top_n=5),
    FlagEmbeddingReranker(model="BAAI/bge-reranker-large", top_n=5),
    SentenceTransformerRerank(model="cross-encoder/ms-marco-MiniLM-L-6-v2", top_n=5)
]

for reranker in rerankers:
    query_engine = index.as_query_engine(
        similarity_top_k=10,
        node_postprocessors=[reranker]
    )
    # Evaluate hit_rate, MRR
```

### 4. Monitor Performance

Track reranking impact:

```python
# Before reranking
retriever_only_results = retriever.retrieve(query)

# After reranking
reranked_results = reranker.postprocess_nodes(retriever_only_results, query)

# Compare relevance, latency, cost
```

### 5. Combine with Hybrid Search

Best production pattern:

```python
# Hybrid retrieval + reranking
hybrid_retriever = QueryFusionRetriever(
    [vector_retriever, bm25_retriever],
    similarity_top_k=10,
    mode="reciprocal_rerank"
)

query_engine = RetrieverQueryEngine.from_args(
    retriever=hybrid_retriever,
    node_postprocessors=[CohereRerank(top_n=5)]
)
```

## Performance Benchmarks

### Top Performing Combinations (from Blog Analysis)

| Embedding | Reranker | Hit Rate | MRR |
|-----------|----------|----------|-----|
| JinaAI Base | bge-reranker-large | 0.938 | 0.869 |
| JinaAI Base | CohereRerank | 0.933 | 0.874 |
| OpenAI | CohereRerank | 0.927 | 0.866 |
| OpenAI | bge-reranker-large | 0.910 | 0.856 |
| Voyage | CohereRerank | 0.916 | 0.851 |

### Key Takeaways

1. **Rerankers are essential**: Nearly all embeddings improve with reranking
2. **Synergistic pairing**: Some embeddings work better with specific rerankers
3. **Foundation matters**: Superior rerankers can't compensate for weak baseline retrieval
4. **Consistency**: Use rerankers consistently across deployments for measurable improvements

## Cost-Performance Trade-offs

### High Performance (API-based)

**CohereRerank**:
- Cost: ~$1 per 1,000 searches (approximate)
- Performance: Best overall
- Latency: 100-300ms

**JinaRerank**:
- Cost: ~$0.50 per 1,000 searches (approximate)
- Performance: Good
- Latency: 100-200ms

### Balanced (Local Models)

**bge-reranker-large**:
- Cost: Free (local)
- Performance: Excellent for open-source
- Latency: 50-150ms (GPU), 200-500ms (CPU)

**SentenceTransformerRerank**:
- Cost: Free (local)
- Performance: Good
- Latency: 30-100ms (GPU), 100-300ms (CPU)

### Budget (Lightweight)

**SentenceTransformerRerank (MiniLM-L-2)**:
- Cost: Free (local)
- Performance: Acceptable
- Latency: 20-50ms (GPU), 50-150ms (CPU)

## Multi-Language Support

### Thai Language (Relevant for iLand)

**CohereRerank Multilingual**:
```python
reranker = CohereRerank(
    top_n=5,
    model="rerank-multilingual-v3.0",  # Supports Thai
    api_key="YOUR_COHERE_API_KEY"
)
```

**Multilingual Cross-Encoders**:
```python
reranker = SentenceTransformerRerank(
    model="cross-encoder/mmarco-mMiniLMv2-L12-H384-v1",  # Multilingual
    top_n=5
)
```

## Relevance to Your Pipelines

### Current Implementation

**src/ (Main Pipeline)**:
- No explicit reranking in current scripts
- Opportunity: Add reranking to all 7 retrieval strategies

**src-iLand/ (iLand Pipeline)**:
- No reranking currently implemented
- Thai content would benefit from multilingual rerankers

### Integration Opportunities

1. **Add Reranking to All Strategies**:
   ```python
   # In each retriever
   query_engine = index.as_query_engine(
       similarity_top_k=10,
       node_postprocessors=[CohereRerank(top_n=5, model="rerank-multilingual-v3.0")]
   )
   ```

2. **Evaluate Reranker Impact**:
   ```python
   # Compare with/without reranking
   strategies = ["vector", "bm25", "hybrid", "metadata"]
   for strategy in strategies:
       evaluate_with_reranking(strategy)
       evaluate_without_reranking(strategy)
   ```

3. **Thai-Optimized Setup**:
   ```python
   # For iLand Thai content
   reranker = CohereRerank(
       top_n=5,
       model="rerank-multilingual-v3.0"
   )
   ```

4. **Cost-Effective Local Option**:
   ```python
   # Alternative to API costs
   reranker = FlagEmbeddingReranker(
       model="BAAI/bge-reranker-large",
       top_n=5
   )
   ```

## Custom Reranker Implementation

```python
from llama_index.core.postprocessor.types import BaseNodePostprocessor
from llama_index.core.schema import NodeWithScore, QueryBundle
from typing import List, Optional

class CustomReranker(BaseNodePostprocessor):
    top_n: int = 5

    def _postprocess_nodes(
        self,
        nodes: List[NodeWithScore],
        query_bundle: Optional[QueryBundle] = None
    ) -> List[NodeWithScore]:
        # Custom reranking logic
        # Example: Score based on metadata, recency, custom model
        scored_nodes = []
        for node in nodes:
            custom_score = self._compute_custom_score(node, query_bundle)
            node.score = custom_score
            scored_nodes.append(node)

        # Sort and return top_n
        scored_nodes.sort(key=lambda x: x.score, reverse=True)
        return scored_nodes[:self.top_n]

    def _compute_custom_score(self, node, query_bundle):
        # Implement custom scoring logic
        pass
```

## References

- [Node Postprocessor Documentation](https://developers.llamaindex.ai/python/framework/module_guides/querying/node_postprocessors/node_postprocessors)
- [Boosting RAG: Embedding & Reranker Models](https://www.llamaindex.ai/blog/boosting-rag-picking-the-best-embedding-reranker-models-42d079022e83)
- [Production RAG Guide](https://developers.llamaindex.ai/python/framework/optimizing/production_rag/)
