# Retrieval Strategies for RAG Systems

## Overview

Retrieval is the process of finding relevant information from indexed documents to answer user queries. LlamaIndex provides multiple retrieval strategies, each optimized for different use cases, query types, and data characteristics.

## Core Retrieval Strategies

### 1. Vector Retrieval (Semantic Search)

**Purpose**: Find semantically similar content using embedding similarity.

**How it works**: Embeds query and documents, then retrieves top-k most similar chunks based on cosine similarity.

```python
from llama_index.core import VectorStoreIndex

index = VectorStoreIndex.from_documents(documents)
retriever = index.as_retriever(similarity_top_k=5)
nodes = retriever.retrieve("What is machine learning?")
```

**Strengths**:
- Semantic understanding beyond keyword matching
- Finds conceptually related content
- Works well for natural language queries

**Limitations**:
- May miss exact keyword matches
- Computationally expensive
- Requires quality embeddings

**Use cases**: General-purpose retrieval, conceptual queries, semantic search

**Relevance to your pipeline**:
- `src/10_basic_query_engine.py` - Basic vector retrieval
- `src-iLand/retrieval/retrievers/vector.py` - iLand vector strategy

### 2. BM25 Retrieval (Keyword Search)

**Purpose**: Rank documents based on query term occurrence and rarity.

**How it works**: Uses TF-IDF with improvements for term frequency saturation and document length normalization.

```python
from llama_index.retrievers.bm25 import BM25Retriever
import Stemmer

bm25_retriever = BM25Retriever.from_defaults(
    nodes=nodes,
    similarity_top_k=5,
    stemmer=Stemmer.Stemmer("english"),
    language="english"
)
```

**Strengths**:
- Fast inference (sub-millisecond)
- Excels at exact keyword matching
- No embedding infrastructure needed
- Minimal computational overhead

**Limitations**:
- Misses semantic similarity
- Language-dependent (requires stemmer)

**Use cases**: Exact term matching, specialized vocabularies, sparse data

**Advanced Features**:

**Disk Persistence**:
```python
bm25_retriever.persist("./bm25_retriever")
loaded_retriever = BM25Retriever.from_persist_dir("./bm25_retriever")
```

**Metadata Filtering**:
```python
from llama_index.core.vector_stores.types import MetadataFilters, MetadataFilter, FilterOperator

filters = MetadataFilters(
    filters=[
        MetadataFilter(key="province", value="กรุงเทพ", operator=FilterOperator.EQ)
    ]
)

retriever = BM25Retriever.from_defaults(
    docstore=docstore,
    similarity_top_k=3,
    filters=filters
)
```

### 3. Hybrid Search

**Purpose**: Combine semantic (vector) and keyword-based (BM25) retrieval for comprehensive coverage.

**Why it matters**: "Embeddings are not perfect, and may fail to return text chunks with matching keywords."

```python
from llama_index.core.retrievers import QueryFusionRetriever

retriever = QueryFusionRetriever(
    [vector_retriever, bm25_retriever],
    similarity_top_k=5,
    num_queries=4,
    mode="reciprocal_rerank",
    use_async=True
)
```

**Strengths**:
- Combines semantic and exact matching
- Best overall performance for mixed queries
- Reduces reliance on single methodology

**Limitations**:
- Increased complexity
- Higher computational cost

**Use cases**: Production RAG systems, complex queries, diverse content types

**Relevance to your pipeline**:
- `src/16_hybrid_search.py` - Hybrid search implementation
- `src-iLand/retrieval/retrievers/hybrid.py` - iLand hybrid strategy

**Native Vector Store Support**:
- Weaviate Hybrid Search
- Pinecone Hybrid Search
- Milvus Hybrid Search
- Qdrant Hybrid Search

### 4. Metadata Filtering

**Purpose**: Attach contextual information to documents for source tracking and runtime filtering.

**How it works**: Filter documents by metadata attributes before or during retrieval.

```python
from llama_index.core import Document
from llama_index.core.vector_stores import MetadataFilters, ExactMatchFilter

documents = [
    Document(text="text", metadata={"author": "LlamaIndex", "date": "2024"}),
    Document(text="text", metadata={"author": "John Doe", "date": "2023"}),
]

filters = MetadataFilters(
    filters=[ExactMatchFilter(key="author", value="John Doe")]
)

index = VectorStoreIndex.from_documents(documents)
query_engine = index.as_query_engine(filters=filters)
```

**Strengths**:
- Precise filtering by attributes
- Fast pre-filtering before vector search
- Reduces search space significantly

**Limitations**:
- Requires structured metadata
- Vector database must support filtering

**Use cases**: Multi-tenant systems, date/category filtering, compliance requirements

**Advanced: AutoVectorRetriever**:
```python
# Uses GPT-4 to automatically generate filters at query time
from llama_index.core.retrievers import AutoVectorRetriever

retriever = AutoVectorRetriever(
    index,
    vector_store_info=vector_store_info
)
```

**Relevance to your pipeline**:
- `src/14_metadata_filtering.py` - Metadata filtering strategy
- `src-iLand/retrieval/retrievers/metadata.py` - Fast metadata filtering (<50ms)
- `src-iLand/retrieval/fast_metadata_index.py` - Inverted & B-tree indices

### 5. Document Summary Retrieval

**Purpose**: Two-stage retrieval using high-level summaries to identify relevant documents before chunk-level search.

**How it works**:
1. Embed document summaries
2. Retrieve relevant documents based on summary similarity
3. Search within selected documents for specific chunks

**Benefits**:
- Reduces irrelevant chunk retrieval
- Better document-level discrimination
- Addresses "lost in the middle" problem

**Implementation Pattern**:
```python
from llama_index.core import DocumentSummaryIndex

# Create index with document summaries
summary_index = DocumentSummaryIndex.from_documents(
    documents,
    response_synthesizer=response_synthesizer
)

retriever = summary_index.as_retriever(similarity_top_k=3)
```

**Strengths**:
- Natural hierarchical structure
- Semantic document-level lookup
- Reduces noise from off-topic documents

**Limitations**:
- Summary generation computational cost
- No keyword filtering at document level

**Use cases**: Large document sets (100+), multi-document queries, hierarchical content

**Relevance to your pipeline**:
- `src/11_document_summary_retriever.py` - Summary-based retrieval
- Pattern applicable to iLand for deed-level summaries

### 6. Recursive Retrieval

**Purpose**: Hierarchical multi-level retrieval with document summaries.

**How it works**: Start with high-level document retrieval, then recursively fetch more detailed chunks.

```python
from llama_index.core.retrievers import RecursiveRetriever
from llama_index.core.query_engine import RetrieverQueryEngine

retriever = RecursiveRetriever(
    "vector",
    retriever_dict={
        "vector": index.as_retriever(),
        "summary": summary_index.as_retriever()
    },
    node_dict=node_dict
)

query_engine = RetrieverQueryEngine.from_args(retriever)
```

**Strengths**:
- Preserves hierarchical structure
- Multi-level context retrieval
- Flexible navigation paths

**Limitations**:
- More complex setup
- Potential for redundant retrievals

**Use cases**: Structured documents, multi-level information, research papers

**Relevance to your pipeline**:
- `src/12_recursive_retriever.py` - Recursive retrieval implementation

### 7. Chunk Decoupling (Sentence-Window)

**Purpose**: Decouple retrieval chunks from synthesis chunks for optimal performance.

**Problem**: "The optimal chunk representation for retrieval might be different than the optimal context used for synthesis."

**How it works**:
- Embed small chunks (sentences) for precise retrieval
- Link to larger context windows for synthesis
- Use MetadataReplacementPostProcessor to swap content

```python
from llama_index.core.node_parser import SentenceWindowNodeParser
from llama_index.core.postprocessor import MetadataReplacementPostProcessor

# Embed sentences for retrieval
node_parser = SentenceWindowNodeParser.from_defaults(
    window_size=3,  # sentences before/after
    window_metadata_key="window"
)

# Replace with window for synthesis
postprocessor = MetadataReplacementPostProcessor(
    target_metadata_key="window"
)

query_engine = index.as_query_engine(
    node_postprocessors=[postprocessor],
    similarity_top_k=6
)
```

**Strengths**:
- Fine-grained retrieval precision
- Rich context for synthesis
- Addresses "lost in the middle" problem

**Limitations**:
- Higher storage requirements
- More complex setup

**Use cases**: Long documents, precise fact retrieval, quality-critical applications

**Relevance to your pipeline**:
- `src/15_chunk_decoupling.py` - Chunk decoupling implementation

### 8. Sub-Question Decomposition

**Purpose**: Decompose complex queries into simpler sub-questions, route to appropriate sources, and synthesize comprehensive answers.

**How it works**:
1. LLM breaks down complex query into sub-questions
2. Route each sub-question to specialized query engine
3. Execute in parallel
4. Synthesize final answer from intermediate results

```python
from llama_index.core.query_engine import SubQuestionQueryEngine
from llama_index.core.tools import QueryEngineTool, ToolMetadata

# Create specialized query engines
query_engine_tools = [
    QueryEngineTool(
        query_engine=essay_engine,
        metadata=ToolMetadata(
            name="essays",
            description="Paul Graham essays collection"
        )
    ),
    QueryEngineTool(
        query_engine=articles_engine,
        metadata=ToolMetadata(
            name="articles",
            description="Technical articles"
        )
    )
]

query_engine = SubQuestionQueryEngine.from_defaults(
    query_engine_tools=query_engine_tools,
    use_async=True
)
```

**Strengths**:
- Handles complex multi-faceted queries
- Parallel sub-question execution
- Multi-source synthesis

**Limitations**:
- Requires multiple LLM calls
- Higher latency and cost
- Depends on decomposition quality

**Use cases**: Comparative questions, multi-source analysis, comprehensive research queries

**Relevance to your pipeline**:
- `src/17_query_planning_agent.py` - Query planning with decomposition
- `src-iLand/retrieval/retrievers/planner.py` - iLand planner strategy

### 9. Reciprocal Rerank Fusion (RRF)

**Purpose**: Rerank results from multiple retrievers without expensive reranker models.

**How it works**:
- Retrieve from multiple systems (vector + BM25)
- Convert scores to reciprocal ranks
- Aggregate ranks across retrievers
- Re-sort by aggregate score

```python
from llama_index.core.retrievers import QueryFusionRetriever

retriever = QueryFusionRetriever(
    [vector_retriever, bm25_retriever],
    similarity_top_k=5,
    num_queries=4,  # Generate query variations
    mode="reciprocal_rerank",
    use_async=True,
    verbose=True
)
```

**Strengths**:
- No additional ML models required
- Lower computational cost than neural reranking
- Effective normalization across different retrievers

**Limitations**:
- Less powerful than neural rerankers
- Depends on retriever diversity

**Use cases**: Hybrid search, cost-efficient reranking, multi-retriever fusion

## Retrieval Configuration Parameters

### Common Parameters

| Parameter | Purpose | Typical Values | Impact |
|-----------|---------|----------------|--------|
| `similarity_top_k` | Number of results | 2-20 | Recall vs. precision |
| `embedding_model` | Vector representation | text-embedding-3-small | Quality, cost |
| `chunk_size` | Document granularity | 256-2048 | Precision vs. context |
| `filters` | Metadata constraints | Various | Search scope |

### Performance Tuning

**Increase Recall**:
- Increase `similarity_top_k`
- Use hybrid search
- Reduce chunk size (compensate with higher top_k)

**Improve Precision**:
- Add metadata filters
- Use rerankers
- Optimize chunk size

**Reduce Latency**:
- Use BM25 for simple queries
- Enable fast metadata pre-filtering
- Reduce `similarity_top_k`

**Lower Cost**:
- Use local embeddings
- Cache retrievals
- Optimize batch sizes

## Advanced Retrieval Patterns

### Router-Based Retrieval

Route queries to appropriate retrieval strategies based on query characteristics:

```python
from llama_index.core.retrievers import RouterRetriever
from llama_index.core.selectors import LLMSingleSelector

router_retriever = RouterRetriever(
    selector=LLMSingleSelector.from_defaults(),
    retriever_tools=[
        RetrieverTool.from_defaults(
            retriever=vector_retriever,
            description="Useful for semantic questions"
        ),
        RetrieverTool.from_defaults(
            retriever=bm25_retriever,
            description="Useful for keyword questions"
        )
    ]
)
```

**Relevance to your pipeline**:
- `src-iLand/retrieval/router.py` - Two-stage routing (index + strategy)
- `src-iLand/retrieval/index_classifier.py` - Routes to appropriate indices

### Agentic Retrieval

LLM-powered retrieval with dynamic tool selection:

```python
from llama_index.core.agent import ReActAgent

agent = ReActAgent.from_tools(
    [vector_tool, metadata_tool, summary_tool],
    llm=llm,
    verbose=True
)
```

**Relevance to your pipeline**:
- `src/agentic_retriever/` - Agentic retrieval implementation
- `src-iLand/retrieval/router.py` - LLM-based strategy selection

## Retrieval Evaluation

### Key Metrics

**Hit Rate**: Fraction of queries where correct answer appears in top-k results.

**Mean Reciprocal Rank (MRR)**: Average of reciprocals of ranks—perfect rank=1.0, degrades with position.

```python
from llama_index.core.evaluation import RetrieverEvaluator

evaluator = RetrieverEvaluator.from_metric_names(
    ["mrr", "hit_rate"],
    retriever=retriever
)

eval_results = await evaluator.aevaluate_dataset(qa_dataset)
```

## Best Practices

### Strategy Selection

1. **Simple factual queries** → Vector or BM25
2. **Complex multi-source queries** → Sub-question decomposition
3. **Production systems** → Hybrid search + reranking
4. **Structured filtering** → Metadata retrieval
5. **Large document sets (100+)** → Document summary or recursive
6. **Quality-critical** → Chunk decoupling with sentence windows
7. **Multi-faceted analysis** → Query planning agent

### Performance Optimization

1. **Pre-filtering**: Use metadata filters to reduce search space
2. **Caching**: Store frequent queries and results
3. **Async operations**: Enable `use_async=True` for parallel retrievals
4. **Batch processing**: Process multiple queries together
5. **Fast metadata indexing**: Use inverted indices for categorical fields

### Production Deployment

1. **Monitoring**: Track retrieval latency, hit rate, MRR
2. **A/B Testing**: Compare strategies on real queries
3. **Fallbacks**: Implement degraded modes for failures
4. **Cost tracking**: Monitor LLM calls and embedding API usage

## Relevance to Your Pipelines

### Current Implementation Analysis

**src/ (Main Pipeline)**:
- 7 retrieval strategies implemented (scripts 10-17)
- Each strategy isolated in separate script
- Opportunity: Unified evaluation across strategies

**src-iLand/ (iLand Pipeline)**:
- Modular retriever architecture (`retrieval/retrievers/`)
- Two-stage routing (index → strategy)
- Fast metadata filtering (<50ms)
- 7 strategies with LLM-based selection

### Integration Opportunities

1. **Unified Evaluation Framework**:
   ```python
   # Test all strategies on same query set
   strategies = [vector, bm25, hybrid, metadata, summary, recursive, planner]
   for strategy in strategies:
       evaluate_strategy(strategy, qa_dataset)
   ```

2. **Strategy Router**:
   ```python
   # Automatic strategy selection based on query
   router = create_strategy_router(strategies)
   response = router.query("What is นส.3?")  # Routes to metadata
   ```

3. **Fast Metadata Pre-filtering**:
   ```python
   # Apply to all strategies
   filtered_nodes = fast_metadata_filter(query, filters)
   results = vector_retriever.retrieve(query, nodes=filtered_nodes)
   ```

## References

- [Basic RAG Optimization Strategies](https://developers.llamaindex.ai/python/framework/optimizing/basic_strategies/basic_strategies/)
- [Production RAG Guide](https://developers.llamaindex.ai/python/framework/optimizing/production_rag/)
- [BM25 Retriever Documentation](https://developers.llamaindex.ai/python/examples/retrievers/bm25_retriever)
- [Reciprocal Rerank Fusion](https://developers.llamaindex.ai/python/examples/retrievers/reciprocal_rerank_fusion)
- [Sub Question Query Engine](https://developers.llamaindex.ai/python/examples/query_engine/sub_question_query_engine)
- [Vector Store Guide](https://developers.llamaindex.ai/python/framework/module_guides/indexing/vector_store_guide)
