## Codebase Patterns
- requirements.txt has all RAG deps: llama-index-retrievers-bm25, qdrant-client, llama-index-vector-stores-qdrant
- .env.example already has QDRANT_PATH=data/qdrant_db and all RAG config vars
- `make lint` runs py_compile on src/rag and src/mmm
- ingest.py provides pre-chunked Documents — do NOT re-split with SentenceSplitter
- Must use `.venv/bin/python` (not system python) to run tests — RAG deps not installed globally
- build_index.py needs `load_dotenv()` at module level to pick up OPENAI_API_KEY from .env
- Qdrant local mode: QdrantClient __del__ warning on shutdown is cosmetic, not a real error
- Sample mode loads 6 files → 694 chunks (~$0.06 estimated cost)
- Full corpus: 28 files → 3435 chunks → 5675 text vectors + 50 asset vectors (~$0.27 estimated cost)

---

## 2026-02-20 - US-001
- Verified all dependencies present in requirements.txt (llama-index-retrievers-bm25, qdrant-client, llama-index-vector-stores-qdrant)
- Verified .env.example documents QDRANT_PATH=data/qdrant_db
- Typecheck passes (make lint)
- No code changes needed — all acceptance criteria already satisfied
- Files changed: none (verification only)
- **Learnings for future iterations:**
  - All RAG deps were already in place from prior work
  - `make lint` only does py_compile, not mypy
---

## 2026-02-20 - US-002 through US-007 (re-validation)
- All code was already implemented from prior iteration (commits 64f0a8b through 817080c)
- Stories reset to passes=false for full end-to-end rerun verification
- All 100 tests pass with .venv/bin/python -m pytest tests/ -v
- Typecheck passes (make lint)
- Files changed: none (re-validation only)
- **Learnings for future iterations:**
  - Prior iteration already built indexer.py, query_engine.py, build_index.py, and all test files
  - Glob tool may not find files when searching from ralph-scripts/ms-02 cwd — use absolute paths or find command
---

## 2026-02-20 - US-008
- Added `load_dotenv()` to build_index.py to load OPENAI_API_KEY from .env
- Ran full dry-run pipeline: estimate, text+BM25 build, asset build, check, smoke queries
- Verified: dry-run prints estimate ($0.059 < $1.00), builds complete successfully
- Verified: text_documents=1465 vectors, campaign_assets=50 vectors, BM25=8 files
- Verified: "What is Meta CPM?" returns meta_ads.csv + config.py sources
- Verified: "DEEPAL S07 launch creative" returns creatives with valid image_path metadata
- Verified: re-running build keeps same vector count (no duplicates)
- All 100 tests pass
- Files changed: src/rag/data_processing/build_index.py (added dotenv import)
- **Learnings for future iterations:**
  - CRITICAL: build_index.py needs load_dotenv() or OPENAI_API_KEY won't be set from .env
  - text-embedding-3-small: sample corpus of 5 files → 644 text chunks = 1465 vectors (LlamaIndex adds metadata nodes)
  - Asset manifest: 50 rows → 50 vectors (1:1, no extra nodes)
  - BM25 persists 8 files including retriever.json, mmindex, etc.
---

## 2026-02-20 - US-009
- Ran full production embedding: all 19 CSVs + 7 contracts + config.py + 50 assets
- Dry-run estimate: 3435 chunks, $0.27 (well under $5.00 cap)
- Built text_documents: 5675 vectors from 3385 chunks (green)
- Built campaign_assets: 50 vectors (green)
- BM25: 8 files, ready
- Smoke queries verified:
  - "What is Meta CPM?" → meta_ads.csv in top-5
  - "TV performance reach" → tv_performance.csv in top-5
  - "DEEPAL S07 launch creative" → valid image_path metadata in top-5
- Re-run verification: vector counts unchanged (no duplicates)
- All 100 tests pass
- Files changed: ralph-scripts/ms-02/prd.json, progress.txt (no code changes)
- **Learnings for future iterations:**
  - Full corpus embeds 19 CSVs + 7 contracts + config = 3385 text chunks → 5675 vectors
  - LlamaIndex creates ~1.67x more vectors than chunks (extra metadata/summary nodes)
  - Re-running build always drops + rebuilds Qdrant collection (clean de-duplication)
  - BM25 loads from persist on second run (doesn't rebuild)
---
